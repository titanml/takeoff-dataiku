{
  "id": "titan-ml-connector",
  "meta": {
    "label": "TitanML Takeoff Server Connector"
  },
  "params": [
    {
      "name": "endpoint_url",
      "label": "Inference Endpoint URL",
      "type": "STRING",
      "description": "The endpoint on which the takeoff server is hosted e.g. http://localhost:3000.",
      "mandatory": true
    },
    {
      "name": "consumer_group",
      "label": "Consumer Group",
      "type": "STRING",
      "description": "The consumer group to send requests to. The default for deployed TitanML models is 'primary', but if you have multiple models on the same server you must assign each to a consumer group. Leave empty to set the default value of 'primary'",
      "mandatory": true
    },
    {
      "name": "jsonSchema",
      "label": "Json Schema",
      "type": "STRING",
      "description": "JSON Schema to constrain every input to",
      "mandatory": false
    },
    {
      "name": "regexScheme",
      "label": "Regex String",
      "type": "STRING",
      "description": "Regex string to constrain every input to.",
      "mandatory": false
    },
    {
      "name": "chatTemplate",
      "label": "Use chat template",
      "type": "BOOLEAN",
      "description": "Whether inputs should be injected into a prompt template. Inputs should follow the message format if so",
      "mandatory": true
    }
  ],
  "clientClass": "com.titanml.llm.TitanMLLLMConnector"
}